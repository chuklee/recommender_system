{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import math\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import requests\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_MOVIELENS = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "URL_IMBD_NAMES_BASICS = \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "URL_IMBD_TITLE_BASICS = \"https://datasets.imdbws.com/title.basics.tsv.gz\"\n",
    "URL_IMBD_TITLE_RATINGS = \"https://datasets.imdbws.com/title.ratings.tsv.gz\"\n",
    "# Local path where the file will be saved\n",
    "LOCAL_MOVIELENS_PATH = \"ml-1m.zip\"\n",
    "LOCAL_IMBD_NAMES_BASICS_PATH = \"name.basics.tsv.gz\"\n",
    "LOCAL_IMBD_TITLE_BASICS_PATH = \"title.basics.tsv.gz\"\n",
    "LOCAL_IMBD_TITLE_RATINGS_PATH = \"title.ratings.tsv.gz\"\n",
    "# Directory where the dataset will be extracted\n",
    "EXTRACT_DIR = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the file\n",
    "def download_file(url, local_filename):\n",
    "    print(f\"Downloading {url} to {local_filename}\")\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(local_filename):\n",
    "        print(f\"File {local_filename} already exists\")\n",
    "        return local_filename\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    print(f\"Downloaded {url} to {local_filename}\")\n",
    "    return local_filename\n",
    "\n",
    "# Function to unzip the file\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    print(f\"Unzipping {zip_path} to {extract_to}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Unzipped {zip_path} to {extract_to}\")\n",
    "\n",
    "def gunzip_file(gz_path, extract_to):\n",
    "    print(f\"Gunzipping {gz_path} to {extract_to}\")\n",
    "    with gzip.open(gz_path, 'rb') as f_in:\n",
    "        with open(extract_to, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Gunzipped {gz_path} to {extract_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://files.grouplens.org/datasets/movielens/ml-1m.zip to ml-1m.zip\n",
      "File ml-1m.zip already exists\n",
      "Downloading https://datasets.imdbws.com/name.basics.tsv.gz to name.basics.tsv.gz\n",
      "File name.basics.tsv.gz already exists\n",
      "Downloading https://datasets.imdbws.com/title.basics.tsv.gz to title.basics.tsv.gz\n",
      "File title.basics.tsv.gz already exists\n",
      "Downloading https://datasets.imdbws.com/title.ratings.tsv.gz to title.ratings.tsv.gz\n",
      "File title.ratings.tsv.gz already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'title.ratings.tsv.gz'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the dataset directory exists\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "# Download the file\n",
    "download_file(URL_MOVIELENS, LOCAL_MOVIELENS_PATH)\n",
    "download_file(URL_IMBD_NAMES_BASICS, LOCAL_IMBD_NAMES_BASICS_PATH)\n",
    "download_file(URL_IMBD_TITLE_BASICS, LOCAL_IMBD_TITLE_BASICS_PATH)\n",
    "download_file(URL_IMBD_TITLE_RATINGS, LOCAL_IMBD_TITLE_RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping file...\n",
      "Unzipping ml-1m.zip to dataset\n",
      "Unzipped ml-1m.zip to dataset\n",
      "Gunzipping files...\n",
      "Gunzipping name.basics.tsv.gz to dataset\\name.basics.tsv\n",
      "Gunzipped name.basics.tsv.gz to dataset\\name.basics.tsv\n",
      "Gunzipping title.basics.tsv.gz to dataset\\title.basics.tsv\n",
      "Gunzipped title.basics.tsv.gz to dataset\\title.basics.tsv\n",
      "Gunzipping title.ratings.tsv.gz to dataset\\title.ratings.tsv\n",
      "Gunzipped title.ratings.tsv.gz to dataset\\title.ratings.tsv\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Extract the files\n",
    "print(\"Unzipping file...\")\n",
    "unzip_file(LOCAL_MOVIELENS_PATH, EXTRACT_DIR)\n",
    "\n",
    "print(\"Gunzipping files...\")\n",
    "gunzip_file(LOCAL_IMBD_NAMES_BASICS_PATH, os.path.join(EXTRACT_DIR, \"name.basics.tsv\"))\n",
    "gunzip_file(LOCAL_IMBD_TITLE_BASICS_PATH, os.path.join(EXTRACT_DIR, \"title.basics.tsv\"))\n",
    "gunzip_file(LOCAL_IMBD_TITLE_RATINGS_PATH, os.path.join(EXTRACT_DIR, \"title.ratings.tsv\"))\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('dataset/ml-1m/movies.dat', sep='::', header=None, engine='python', names=['movie_id', 'title', 'genres'], encoding='ISO-8859-1')\n",
    "ratings = pd.read_csv('dataset/ml-1m/ratings.dat', sep='::', header=None, engine='python', names=['user_id', 'movie_id', 'rating', 'unix_timestamp'], encoding='ISO-8859-1')\n",
    "users = pd.read_csv('dataset/ml-1m/users.dat', sep='::', header=None, engine='python', names=['user_id', 'sex', 'age_group', 'occupation', 'zip_code'], encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do some simple data processing to fix the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
    "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
    "\n",
    "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "\n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_user_1</td>\n",
       "      <td>F</td>\n",
       "      <td>group_group_1</td>\n",
       "      <td>occupation_occupation_10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_user_2</td>\n",
       "      <td>M</td>\n",
       "      <td>group_group_56</td>\n",
       "      <td>occupation_occupation_16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_user_3</td>\n",
       "      <td>M</td>\n",
       "      <td>group_group_25</td>\n",
       "      <td>occupation_occupation_15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_user_4</td>\n",
       "      <td>M</td>\n",
       "      <td>group_group_45</td>\n",
       "      <td>occupation_occupation_7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_user_5</td>\n",
       "      <td>M</td>\n",
       "      <td>group_group_25</td>\n",
       "      <td>occupation_occupation_20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>user_user_6036</td>\n",
       "      <td>F</td>\n",
       "      <td>group_group_25</td>\n",
       "      <td>occupation_occupation_15</td>\n",
       "      <td>32603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>user_user_6037</td>\n",
       "      <td>F</td>\n",
       "      <td>group_group_45</td>\n",
       "      <td>occupation_occupation_1</td>\n",
       "      <td>76006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>user_user_6038</td>\n",
       "      <td>F</td>\n",
       "      <td>group_group_56</td>\n",
       "      <td>occupation_occupation_1</td>\n",
       "      <td>14706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>user_user_6039</td>\n",
       "      <td>F</td>\n",
       "      <td>group_group_45</td>\n",
       "      <td>occupation_occupation_0</td>\n",
       "      <td>01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>user_user_6040</td>\n",
       "      <td>M</td>\n",
       "      <td>group_group_25</td>\n",
       "      <td>occupation_occupation_6</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id sex       age_group                occupation zip_code\n",
       "0        user_user_1   F   group_group_1  occupation_occupation_10    48067\n",
       "1        user_user_2   M  group_group_56  occupation_occupation_16    70072\n",
       "2        user_user_3   M  group_group_25  occupation_occupation_15    55117\n",
       "3        user_user_4   M  group_group_45   occupation_occupation_7    02460\n",
       "4        user_user_5   M  group_group_25  occupation_occupation_20    55455\n",
       "...              ...  ..             ...                       ...      ...\n",
       "6035  user_user_6036   F  group_group_25  occupation_occupation_15    32603\n",
       "6036  user_user_6037   F  group_group_45   occupation_occupation_1    76006\n",
       "6037  user_user_6038   F  group_group_56   occupation_occupation_1    14706\n",
       "6038  user_user_6039   F  group_group_45   occupation_occupation_0    01060\n",
       "6039  user_user_6040   M  group_group_25   occupation_occupation_6    11106\n",
       "\n",
       "[6040 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each movie has multiple genres. We split them into separate columns in the movies DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = set()\n",
    "for actual_genres in movies['genres'].str.split('|'):\n",
    "    genres.update(actual_genres)\n",
    "genres = list(genres)\n",
    "for genre in genres:\n",
    "    movies[genre] = movies[\"genres\"].apply(\n",
    "        lambda values: int(genre in values.split(\"|\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the movie ratings data into sequences\n",
    "First, let's sort the the ratings data using the unix_timestamp, and then group the movie_id values and the rating values by user_id.\n",
    "\n",
    "The output DataFrame will have a record for each user_id, with two ordered lists (sorted by rating datetime): the movies they have rated, and their ratings of these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(ratings_group.groups.keys()),\n",
    "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
    "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
    "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the movie_ids list into a set of sequences of a fixed length. We do the same for the ratings. Set the sequence_length variable to change the length of the input sequence to the model. You can also change the step_size to control the number of sequences to generate for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "step_size = 2\n",
    "\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"timestamps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we process the output to have each sequence in a separate records in the DataFrame. In addition, we join the user features with the ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
    "    \"movie_ids\", ignore_index=True\n",
    ")\n",
    "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
    "ratings_data_transformed = ratings_data_transformed.join(\n",
    "    users.set_index(\"user_id\"), on=\"user_id\"\n",
    ")\n",
    "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
    "    lambda x: \",\".join(x)\n",
    ")\n",
    "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
    "    lambda x: \",\".join([str(v) for v in x])\n",
    ")\n",
    "\n",
    "del ratings_data_transformed[\"zip_code\"]\n",
    "\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sequence_length of 4 and step_size of 2, we end up with 498,623 sequences.\n",
    "\n",
    "Finally, we split the data into training and testing splits, with 85% and 15% of the instances, respectively, and store them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
    "train_data = ratings_data_transformed[random_selection]\n",
    "test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "train_data.to_csv(os.path.join(EXTRACT_DIR, \"train_data.csv\"), index=False, sep=\"|\", header=False)\n",
    "test_data.to_csv(os.path.join(EXTRACT_DIR, \"test_data.csv\"), index=False, sep=\"|\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"user_id\": list(users.user_id.unique()),\n",
    "    \"movie_id\": list(movies.movie_id.unique()),\n",
    "    \"sex\": list(users.sex.unique()),\n",
    "    \"age_group\": list(users.age_group.unique()),\n",
    "    \"occupation\": list(users.occupation.unique()),\n",
    "}\n",
    "\n",
    "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
    "\n",
    "MOVIE_FEATURES = [\"genres\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf.data.Dataset for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
    "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
    "\n",
    "        # The last movie id in the sequence is the target movie.\n",
    "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
    "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
    "\n",
    "        ratings_string = features[\"sequence_ratings\"]\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        # The last rating in the sequence is the target for the model to predict.\n",
    "        target = sequence_ratings[:, -1]\n",
    "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
    "        \"sequence_movie_ids\": keras.Input(\n",
    "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
    "        ),\n",
    "        \"target_movie_id\": keras.Input(\n",
    "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
    "        ),\n",
    "        \"sequence_ratings\": keras.Input(\n",
    "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        ),\n",
    "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
    "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
    "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode input features\n",
    "The encode_input_features method works as follows:\n",
    "\n",
    "1. Each categorical user feature is encoded using layers.Embedding, with embedding dimension equals to the square root of the vocabulary size of the feature. The embeddings of these features are concatenated to form a single input tensor.\n",
    "\n",
    "2. Each movie in the movie sequence and the target movie is encoded layers.Embedding, where the dimension size is the square root of the number of movies.\n",
    "\n",
    "3. A multi-hot genres vector for each movie is concatenated with its embedding vector, and processed using a non-linear layers.Dense to output a vector of the same movie embedding dimensions.\n",
    "\n",
    "4. A positional embedding is added to each movie embedding in the sequence, and then multiplied by its rating from the ratings sequence.\n",
    "\n",
    "5. The target movie embedding is concatenated to the sequence movie embeddings, producing a tensor with the shape of [batch size, sequence length, embedding size], as expected by the attention layer for the transformer architecture.\n",
    "\n",
    "6. The method returns a tuple of two elements: encoded_transformer_features and encoded_other_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_features(\n",
    "    inputs,\n",
    "    include_user_id=True,\n",
    "    include_user_features=True,\n",
    "    include_movie_features=True,\n",
    "):\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"user_id\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "    ## Encode user features\n",
    "    for feature_name in other_feature_names:\n",
    "        # Convert the string input values into integer indices.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        # Compute embedding dimensions\n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "\n",
    "    ## Create a single embedding vector for the user features\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    ## Create a movie embedding encoder\n",
    "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
    "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
    "    # Create a lookup to convert string values to integer indices.\n",
    "    movie_index_lookup = StringLookup(\n",
    "        vocabulary=movie_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"movie_index_lookup\",\n",
    "    )\n",
    "    # Create an embedding layer with the specified dimensions.\n",
    "    movie_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(movie_vocabulary),\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=f\"movie_embedding\",\n",
    "    )\n",
    "    # Create a vector lookup for movie genres.\n",
    "    genre_vectors = movies[genres].to_numpy()\n",
    "    movie_genres_lookup = layers.Embedding(\n",
    "        input_dim=genre_vectors.shape[0],\n",
    "        output_dim=genre_vectors.shape[1],\n",
    "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
    "        trainable=False,\n",
    "        name=\"genres_vector\",\n",
    "    )\n",
    "    # Create a processing layer for genres.\n",
    "    movie_embedding_processor = layers.Dense(\n",
    "        units=movie_embedding_dims,\n",
    "        activation=\"relu\",\n",
    "        name=\"process_movie_embedding_with_genres\",\n",
    "    )\n",
    "\n",
    "    ## Define a function to encode a given movie id.\n",
    "    def encode_movie(movie_id):\n",
    "        # Convert the string input values into integer indices.\n",
    "        movie_idx = movie_index_lookup(movie_id)\n",
    "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
    "        encoded_movie = movie_embedding\n",
    "        if include_movie_features:\n",
    "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
    "            encoded_movie = movie_embedding_processor(\n",
    "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
    "            )\n",
    "        return encoded_movie\n",
    "\n",
    "    ## Encoding target_movie_id\n",
    "    target_movie_id = inputs[\"target_movie_id\"]\n",
    "    encoded_target_movie = encode_movie(target_movie_id)\n",
    "\n",
    "    ## Encoding sequence movie_ids.\n",
    "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
    "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
    "    # Create positional embedding.\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
    "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
    "    sequence_ratings = keras.ops.expand_dims(sequence_ratings, -1)\n",
    "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
    "    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    # Construct the transformer inputs.\n",
    "    for i in range(sequence_length - 1):\n",
    "        feature = encoded_sequence_movies_with_poistion_and_rating[:, i, ...]\n",
    "        feature = keras.ops.expand_dims(feature, 1)\n",
    "        encoded_transformer_features.append(feature)\n",
    "    encoded_transformer_features.append(encoded_target_movie)\n",
    "\n",
    "    encoded_transformer_features = layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Children's\",\n",
       "  'Comedy',\n",
       "  'Romance',\n",
       "  'Western',\n",
       "  'Horror',\n",
       "  'Sci-Fi',\n",
       "  'Documentary',\n",
       "  'War',\n",
       "  'Animation',\n",
       "  'Mystery',\n",
       "  'Adventure',\n",
       "  'Fantasy',\n",
       "  'Musical',\n",
       "  'Action',\n",
       "  'Drama',\n",
       "  'Thriller',\n",
       "  'Film-Noir',\n",
       "  'Crime']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_user_id = False\n",
    "include_user_features = False\n",
    "include_movie_features = False\n",
    "\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_movie_features\n",
    "    )\n",
    "\n",
    "    # Create a multi-headed attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    # Transformer block.\n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = layers.Add()([transformer_features, attention_output])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x2 = layers.LeakyReLU()(x1)\n",
    "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = layers.Add()([x1, x2])\n",
    "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "    features = layers.Flatten()(transformer_features)\n",
    "\n",
    "    # Included the other features.\n",
    "    if other_features is not None:\n",
    "        features = layers.concatenate(\n",
    "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    # Fully-connected layers.\n",
    "    for num_units in hidden_units:\n",
    "        features = layers.Dense(num_units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.LeakyReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training and evaluation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1603/1603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 16ms/step - loss: 1.6734 - mean_absolute_error: 1.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.799\n"
     ]
    }
   ],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "# Read the training data.\n",
    "train_dataset = get_dataset_from_csv(os.path.join(EXTRACT_DIR, \"train_data.csv\"), shuffle=True, batch_size=265)\n",
    "\n",
    "# Fit the model with the training data.\n",
    "model.fit(train_dataset, epochs=1)\n",
    "\n",
    "# Read the test data.\n",
    "test_dataset = get_dataset_from_csv(os.path.join(EXTRACT_DIR, \"test_data.csv\"), batch_size=265)\n",
    "\n",
    "# Evaluate the model on the test data.\n",
    "_, rmse = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Test MAE: {round(rmse, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now make prediction of the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to get predictions.\n",
    "predictions = model.predict(test_dataset)\n",
    "print(predictions[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
